/**
 * LLM-as-Judge è¯„ä¼°å™¨
 * ä½¿ç”¨ LLM æ¥è¯„ä¼° Agent çš„ä»»åŠ¡å®Œæˆåº¦
 */

import { useAIStore } from '@/stores/useAIStore';
import { TestCase } from './testCases';
import { AgentResult, MetricResult } from './types';

export interface LLMEvalResult {
  taskCompletion: MetricResult;
  toolCorrectness: MetricResult;
  planQuality: MetricResult;
  outputQuality: MetricResult;
  overallScore: number;
  llmReasoning: string;
  // è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæŠ¥å‘Šï¼‰
  evalPrompt: string;
  llmRawResponse: string;
  rawScores: {
    taskCompletion: { score: number; reason: string };
    toolCorrectness: { score: number; reason: string };
    planQuality: { score: number; reason: string };
    outputQuality: { score: number; reason: string };
  };
}

/**
 * è°ƒç”¨ LLM API è¿›è¡Œè¯„ä¼°
 */
async function callLLMForEval(prompt: string): Promise<string> {
  const config = useAIStore.getState().config;
  
  if (!config.apiKey) {
    throw new Error('æœªé…ç½® API Key');
  }

  const baseUrl = config.baseUrl || 'https://api.openai.com/v1';
  const model = config.model === 'custom' ? config.customModelId : config.model;

  const response = await fetch(`${baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify({
      model: model || 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI Agent è¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°ä¸€ä¸ªç¬”è®°ç®¡ç† Agent çš„ä»»åŠ¡å®Œæˆæƒ…å†µã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§è¯„ä¼°æ ‡å‡†æ‰“åˆ†ï¼Œç»™å‡º 0-100 çš„åˆ†æ•°å’Œè¯¦ç»†ç†ç”±ã€‚
è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
  "task_completion": { "score": 0-100, "reason": "ç†ç”±" },
  "tool_correctness": { "score": 0-100, "reason": "ç†ç”±" },
  "plan_quality": { "score": 0-100, "reason": "ç†ç”±" },
  "output_quality": { "score": 0-100, "reason": "ç†ç”±" },
  "overall_reasoning": "æ•´ä½“è¯„ä»·"
}`
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.1,
      max_tokens: 1000,
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`LLM API è°ƒç”¨å¤±è´¥: ${error}`);
  }

  const data = await response.json();
  return data.choices[0]?.message?.content || '';
}

/**
 * æ„å»ºè¯„ä¼° Prompt
 */
function buildEvalPrompt(testCase: TestCase, result: AgentResult): string {
  const toolsCalled = result.toolsCalled.map(t => 
    `- ${t.name}(${JSON.stringify(t.params).slice(0, 100)}...) â†’ ${t.success ? 'æˆåŠŸ' : 'å¤±è´¥'}`
  ).join('\n');

  const planSteps = result.plan?.steps.map(s => 
    `- [${s.completed ? 'âœ“' : ' '}] ${s.description}`
  ).join('\n') || '(æ— è®¡åˆ’)';

  return `
## è¯„ä¼°ä»»åŠ¡

### æµ‹è¯•ç”¨ä¾‹
- **ID**: ${testCase.id}
- **ç±»åˆ«**: ${testCase.category}
- **åç§°**: ${testCase.name}

### ç”¨æˆ·è¾“å…¥
${testCase.input}

### è¯„ä¼°æ ‡å‡†
${testCase.evaluationCriteria?.join('\n') || 'å®Œæˆç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡'}

### æœŸæœ›å·¥å…·
${testCase.expectedTools?.join(', ') || '(æœªæŒ‡å®š)'}

---

## Agent æ‰§è¡Œç»“æœ

### æœ€ç»ˆçŠ¶æ€
${result.finalStatus}

### æ‰§è¡Œè®¡åˆ’
${planSteps}

### å·¥å…·è°ƒç”¨è®°å½•
${toolsCalled || '(æ— å·¥å…·è°ƒç”¨)'}

### Agent è¾“å‡º
${result.actualOutput?.slice(0, 500) || '(æ— è¾“å‡º)'}

### Token ä½¿ç”¨
- Prompt: ${result.tokenUsage.prompt}
- Completion: ${result.tokenUsage.completion}
- Total: ${result.tokenUsage.total}

### æ‰§è¡Œæ—¶é—´
${result.completionTimeMs}ms

---

## è¯„ä¼°æ ‡å‡†

1. **ä»»åŠ¡å®Œæˆåº¦ (task_completion)**: Agent æ˜¯å¦æ­£ç¡®ç†è§£å¹¶å®Œæˆäº†ç”¨æˆ·çš„è¯·æ±‚ï¼Ÿ
2. **å·¥å…·æ­£ç¡®æ€§ (tool_correctness)**: Agent æ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„å·¥å…·ï¼Ÿå·¥å…·è°ƒç”¨æ˜¯å¦æˆåŠŸï¼Ÿ
3. **è®¡åˆ’è´¨é‡ (plan_quality)**: è®¡åˆ’æ˜¯å¦åˆç†ã€å®Œæ•´ã€æ­¥éª¤æ¸…æ™°ï¼Ÿ
4. **è¾“å‡ºè´¨é‡ (output_quality)**: æœ€ç»ˆè¾“å‡ºæ˜¯å¦æœ‰å¸®åŠ©ã€å‡†ç¡®ã€æ ¼å¼è‰¯å¥½ï¼Ÿ

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œè¯„ä¼°ï¼Œè¾“å‡º JSON æ ¼å¼çš„è¯„åˆ†ç»“æœã€‚
`;
}

/**
 * è§£æ LLM è¯„ä¼°ç»“æœ
 */
function parseEvalResult(llmResponse: string, evalPrompt: string): LLMEvalResult | null {
  try {
    // æå– JSON
    const jsonMatch = llmResponse.match(/\{[\s\S]*\}/);
    if (!jsonMatch) return null;

    const parsed = JSON.parse(jsonMatch[0]);

    const toMetric = (name: string, data: { score: number; reason: string }): MetricResult => ({
      name,
      score: (data.score || 0) / 100,
      passed: (data.score || 0) >= 70,
      reason: data.reason || '',
    });

    const rawScores = {
      taskCompletion: parsed.task_completion || { score: 0, reason: 'è§£æå¤±è´¥' },
      toolCorrectness: parsed.tool_correctness || { score: 0, reason: 'è§£æå¤±è´¥' },
      planQuality: parsed.plan_quality || { score: 0, reason: 'è§£æå¤±è´¥' },
      outputQuality: parsed.output_quality || { score: 0, reason: 'è§£æå¤±è´¥' },
    };

    return {
      taskCompletion: toMetric('task_completion', rawScores.taskCompletion),
      toolCorrectness: toMetric('tool_correctness', rawScores.toolCorrectness),
      planQuality: toMetric('plan_quality', rawScores.planQuality),
      outputQuality: toMetric('output_quality', rawScores.outputQuality),
      overallScore: (
        (rawScores.taskCompletion.score || 0) +
        (rawScores.toolCorrectness.score || 0) +
        (rawScores.planQuality.score || 0) +
        (rawScores.outputQuality.score || 0)
      ) / 400,
      llmReasoning: parsed.overall_reasoning || '',
      evalPrompt,
      llmRawResponse: llmResponse,
      rawScores,
    };
  } catch (e) {
    console.error('è§£æ LLM è¯„ä¼°ç»“æœå¤±è´¥:', e);
    return null;
  }
}

/**
 * ä½¿ç”¨ LLM è¯„ä¼° Agent ç»“æœ
 */
export async function evaluateWithLLM(
  testCase: TestCase,
  result: AgentResult
): Promise<LLMEvalResult> {
  console.log(`ğŸ¤– [LLM Eval] è¯„ä¼°æµ‹è¯•: ${testCase.id}`);
  
  const prompt = buildEvalPrompt(testCase, result);
  
  try {
    const llmResponse = await callLLMForEval(prompt);
    
    console.log(`ğŸ¤– [LLM Eval] æ”¶åˆ°å“åº”`);
    
    const evalResult = parseEvalResult(llmResponse, prompt);
    
    if (evalResult) {
      console.log(`ğŸ¤– [LLM Eval] è¯„åˆ†: ${(evalResult.overallScore * 100).toFixed(1)}%`);
      return evalResult;
    }
    
    // è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
    return getDefaultEvalResult('LLM å“åº”è§£æå¤±è´¥', prompt, llmResponse);
  } catch (error) {
    console.error(`ğŸ¤– [LLM Eval] è¯„ä¼°å¤±è´¥:`, error);
    return getDefaultEvalResult(String(error), prompt, '');
  }
}

/**
 * è·å–é»˜è®¤è¯„ä¼°ç»“æœï¼ˆå½“ LLM è¯„ä¼°å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
 */
function getDefaultEvalResult(reason: string, evalPrompt: string, llmRawResponse: string): LLMEvalResult {
  const defaultMetric = (name: string): MetricResult => ({
    name,
    score: 0,
    passed: false,
    reason: `LLM è¯„ä¼°å¤±è´¥: ${reason}`,
  });

  const defaultScore = { score: 0, reason: `LLM è¯„ä¼°å¤±è´¥: ${reason}` };

  return {
    taskCompletion: defaultMetric('task_completion'),
    toolCorrectness: defaultMetric('tool_correctness'),
    planQuality: defaultMetric('plan_quality'),
    outputQuality: defaultMetric('output_quality'),
    overallScore: 0,
    llmReasoning: reason,
    evalPrompt,
    llmRawResponse,
    rawScores: {
      taskCompletion: defaultScore,
      toolCorrectness: defaultScore,
      planQuality: defaultScore,
      outputQuality: defaultScore,
    },
  };
}

/**
 * æ£€æŸ¥æ˜¯å¦å¯ç”¨ LLM è¯„ä¼°
 */
export function isLLMEvalEnabled(): boolean {
  const config = useAIStore.getState().config;
  return !!config.apiKey;
}
